{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the model.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import argparse\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from framework.dataset import LandCoverData as LCD\n",
    "from framework.dataset import parse_image, load_image_train, load_image_test\n",
    "from framework.model import UNet\n",
    "from framework.tensorflow_utils import plot_predictions\n",
    "from framework.utils import YamlNamespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tester Unet++ : https://github.com/MrGiovanni/UNetPlusPlus/tree/master/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"A callback used to display sample predictions during training.\"\"\"\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    def __init__(self, dataset: tf.data.Dataset=None,\n",
    "                 sample_batch: tf.Tensor=None,\n",
    "                 save_folder: Path=None,\n",
    "                 num: int=1,\n",
    "                 ipython_mode: bool=False):\n",
    "        super(PlotCallback, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.sample_batch = sample_batch\n",
    "        self.save_folder = save_folder\n",
    "        self.num = num\n",
    "        self.ipython_mode = ipython_mode\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if self.ipython_mode:\n",
    "            self.clear_output(wait=True)\n",
    "        if self.save_folder:\n",
    "            save_filepaths = [self.save_folder/f'plot_{n}_epoch{epoch}.png' for n in range(1, self.num+1)]\n",
    "        else:\n",
    "            save_filepaths = None\n",
    "        plot_predictions(self.model, self.dataset, self.sample_batch, num=self.num, save_filepaths=save_filepaths)\n",
    "\n",
    "\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser('Training script')\n",
    "    parser.add_argument('--config', '-c', type=str, required=True, help=\"The YAML config file\")\n",
    "    cli_args = parser.parse_args()\n",
    "    # parse the config file\n",
    "    with open(cli_args.config, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    config = YamlNamespace(config)\n",
    "    config.xp_rootdir = Path(config.xp_rootdir).expanduser()\n",
    "    assert config.xp_rootdir.is_dir()\n",
    "    config.dataset_folder = Path(config.dataset_folder).expanduser()\n",
    "    assert config.dataset_folder.is_dir()\n",
    "    if config.val_samples_csv is not None:\n",
    "        config.val_samples_csv = Path(config.val_samples_csv).expanduser()\n",
    "        assert config.val_samples_csv.is_file()\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# the size of each batch of images when training\n",
    "# careful not to use a too large batch size, might lead to OOM errors\n",
    "batch_size = 8\n",
    "# the number of epochs to train for\n",
    "epochs = 90\n",
    "# the learning rate\n",
    "lr = 0.001\n",
    "# the path to the root directory of the dataset\n",
    "dataset_folder = Path('/home/homer/Documents/Master_Statistique/Apprentissage_automatique/challenge/benchmark/challenge-ens/data/dataset').expanduser()\n",
    "# the path to the root directory used to store experiments\n",
    "# a directory per experiment will be created named with the datetime during execution\n",
    "xp_rootdir = Path('/home/homer/Documents/Master_Statistique/Apprentissage_automatique/challenge/benchmark/challenge-ens/code/experiments').expanduser()\n",
    "# CSV file containing the samples to use in the validation set\n",
    "# if null, a random train/val split is performed, with 10% of the samples held out for validation\n",
    "# the validation samples will be saved to a file 'val_samples.csv' in the experiment directory\n",
    "val_samples_csv = None\n",
    "if val_samples_csv is not None:\n",
    "    val_samples_csv = Path(config.val_samples_csv).expanduser()\n",
    "\n",
    "config = {'seed':seed, 'batch_size':batch_size, 'lr':lr, 'dataset_folder':dataset_folder, 'xp_rootdir':xp_rootdir, 'val_samples_csv':val_samples_csv, 'epochs':epochs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'seed': 42, 'batch_size': 8, 'lr': 0.001, 'dataset_folder': PosixPath('/home/homer/Documents/Master_Statistique/Apprentissage_automatique/challenge/benchmark/challenge-ens/data/dataset'), 'xp_rootdir': PosixPath('/home/homer/Documents/Master_Statistique/Apprentissage_automatique/challenge/benchmark/challenge-ens/code/experiments'), 'val_samples_csv': None, 'epochs': 90}\n",
      "Instanciate train and validation datasets\n"
     ]
    }
   ],
   "source": [
    "n_train = 10000 #if null take all train images\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "print(f'Config:\\n{config}')\n",
    "# set random seed for reproducibility\n",
    "if config['seed']is not None:\n",
    "    random.seed(config['seed'])\n",
    "    np.random.seed(config['seed'])\n",
    "    tf.random.set_seed(config['seed'])\n",
    "\n",
    "N_CPUS = multiprocessing.cpu_count()\n",
    "\n",
    "print('Instanciate train and validation datasets')\n",
    "train_files = list(config['dataset_folder'].glob('train/images/*.tif'))\n",
    "# shuffle list of training samples files\n",
    "if n_train :\n",
    "    train_files = random.sample(train_files, n_train)\n",
    "else :\n",
    "    train_files = random.sample(train_files, len(train_files))\n",
    "devset_size = len(train_files)\n",
    "# validation set\n",
    "if config['val_samples_csv'] is not None:\n",
    "    # read the validation samples\n",
    "    val_samples_s = pd.read_csv(config['val_samples_csv'], squeeze=True)\n",
    "    val_files = [config['dataset_folder']/'train/images/{}.tif'.format(i) for i in val_samples_s]\n",
    "    train_files = [f for f in train_files if f not in set(val_files)]\n",
    "    valset_size = len(val_files)\n",
    "    trainset_size = len(train_files)\n",
    "    assert valset_size + trainset_size == devset_size\n",
    "else:\n",
    "    # generate a hold-out validation set from the training set\n",
    "    valset_size = int(len(train_files) * 0.1)\n",
    "    train_files, val_files = train_files[valset_size:], train_files[:valset_size]\n",
    "    trainset_size = len(train_files) - valset_size\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(list(map(str, train_files)))\\\n",
    "    .map(parse_image, num_parallel_calls=N_CPUS)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(list(map(str, val_files)))\\\n",
    "    .map(parse_image, num_parallel_calls=N_CPUS)\n",
    "\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=N_CPUS)\\\n",
    "    .shuffle(buffer_size=1024, seed=config['seed'])\\\n",
    "    .repeat()\\\n",
    "    .batch(config['batch_size'])\\\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_dataset.map(load_image_test, num_parallel_calls=N_CPUS)\\\n",
    "    .repeat()\\\n",
    "    .batch(config['batch_size'])\\\n",
    "    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# where to write files for this experiments\n",
    "xp_dir = config['xp_rootdir'] / datetime.datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "(xp_dir/'tensorboard').mkdir(parents=True)\n",
    "(xp_dir/'plots').mkdir()\n",
    "(xp_dir/'checkpoints').mkdir()\n",
    "# save the validation samples to a CSV\n",
    "val_samples_s = pd.Series([int(f.stem) for f in val_files], name='sample_id', dtype='uint32')\n",
    "val_samples_s.to_csv(xp_dir/'val_samples.csv', index=False)\n",
    "\n",
    "# keep a training minibatch for visualization\n",
    "for image, mask in train_dataset.take(1):\n",
    "    sample_batch = (image[:5, ...], mask[:5, ...])\n",
    "\n",
    "callbacks = [\n",
    "    PlotCallback(sample_batch=sample_batch, save_folder=xp_dir/'plots', num=5),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=xp_dir/'tensorboard',\n",
    "        update_freq='epoch'\n",
    "    ),\n",
    "    # tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=xp_dir/'checkpoints/epoch{epoch}', save_best_only=False, verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        filename=(xp_dir/'fit_logs.csv')\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        patience=20,\n",
    "        factor=0.5,\n",
    "        verbose=1,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating U-Net with arguments: {'input_shape': (256, 256, 4), 'num_classes': 10, 'num_layers': 1}\n",
      "Model: \"unet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 64) 2368        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 256, 256, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 36928       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 64) 36928       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 128, 128, 64) 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 64) 36928       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 64)   36928       batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 128, 128, 64) 36928       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128, 128, 128 0           conv2d_transpose[0][0]           \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 128, 128 512         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 128, 128, 96) 110688      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 128, 128, 96) 384         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 128, 128, 64) 55360       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128, 128, 64) 256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 256, 256, 64) 36928       batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256, 256, 128 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 256, 256, 128 512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 256, 256, 96) 110688      batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 256, 256, 96) 384         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 256, 64) 55360       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 256, 10) 650         conv2d_11[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 671,562\n",
      "Trainable params: 669,514\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Will use class weights: [0.00000000e+00 0.00000000e+00 5.46819219e-01 1.57013485e+00\n",
      " 4.54326487e-01 1.54957668e+01 1.41433886e+05 5.15142753e+00]\n",
      "Compile model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/homer/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create the U-Net model to train\n",
    "unet_kwargs = dict(\n",
    "    input_shape=(LCD.IMG_SIZE, LCD.IMG_SIZE, LCD.N_CHANNELS),\n",
    "    num_classes=LCD.N_CLASSES,\n",
    "    num_layers=1\n",
    ")\n",
    "print(f\"Creating U-Net with arguments: {unet_kwargs}\")\n",
    "model = UNet(**unet_kwargs)\n",
    "print(model.summary())\n",
    "\n",
    "# get optimizer, loss, and compile model for training\n",
    "optimizer = tf.keras.optimizers.Adam(lr=config['lr'])\n",
    "\n",
    "# compute class weights for the loss: inverse-frequency balanced\n",
    "# note: we set to 0 the weights for the classes \"no_data\"(0) and \"clouds\"(1) to ignore these\n",
    "class_weight = (1 / LCD.TRAIN_CLASS_COUNTS[2:])* LCD.TRAIN_CLASS_COUNTS[2:].sum() / (LCD.N_CLASSES-2)\n",
    "class_weight[LCD.IGNORED_CLASSES_IDX] = 0.\n",
    "print(f\"Will use class weights: {class_weight}\")\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "print(\"Compile model\")\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])\n",
    "              # metrics = [tf.keras.metrics.Precision(),\n",
    "              #            tf.keras.metrics.Recall(),\n",
    "              #            tf.keras.metrics.MeanIoU(num_classes=LCD.N_CLASSES)]) # TODO segmentation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "  44/1000 [>.............................] - ETA: 5:55:23 - loss: 1.1157 - accuracy: 0.5740"
     ]
    }
   ],
   "source": [
    "# Launch training\n",
    "model_history = model.fit(train_dataset, epochs=config['epochs'],\n",
    "                          callbacks=callbacks,\n",
    "                          steps_per_epoch=trainset_size // config['batch_size'],\n",
    "                          validation_data=val_dataset,\n",
    "                          validation_steps=valset_size // config['batch_size'],\n",
    "                          #class_weight=class_weight\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_dico = {i:class_weight[i] for i in range(len(class_weight))}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
